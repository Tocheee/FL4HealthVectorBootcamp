# Parameters that describe server
n_server_rounds: 15 # The number of rounds to run FL

# Parameters that describe clients
n_clients: 4 # The number of clients in the FL experiment
local_steps: 500 # The number of epochs to complete for client
batch_size: 1 # The batch size for client training

evaluate_after_fit: false # Whether to evaluate the model after training√•


dataset:
  name: "vicgalle/alpaca-gpt4"

model:
  name: "openlm-research/open_llama_3b_v2"
  quantization: 4
  gradient_checkpointing: true
  lora:
    peft_lora_r: 32
    peft_lora_alpha: 64

train:
  save_every_round: 5
  learning_rate_max: 5e-5
  learning_rate_min: 1e-6
  seq-length: 512
  training_arguments:
    gradient_accumulation_steps: 1
    logging_steps: 10
    save_steps: 500
    save_total_limit: 10
    gradient_checkpointing: true
    lr_scheduler_type: "constant"
