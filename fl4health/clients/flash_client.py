from logging import INFO
from pathlib import Path
from typing import Dict, Optional, Sequence, Tuple

import torch
from flwr.common.logger import log
from flwr.common.typing import Config, Scalar

from fl4health.checkpointing.client_module import ClientCheckpointModule
from fl4health.clients.basic_client import BasicClient
from fl4health.utils.losses import LossMeterType
from fl4health.utils.metrics import Metric


class FlashClient(BasicClient):
    def __init__(
        self,
        data_path: Path,
        metrics: Sequence[Metric],
        device: torch.device,
        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,
        checkpointer: Optional[ClientCheckpointModule] = None,
    ) -> None:
        super().__init__(
            data_path=data_path,
            metrics=metrics,
            device=device,
            loss_meter_type=loss_meter_type,
            checkpointer=checkpointer,
        )
        self.gamma: Optional[float] = None

    def train_by_epochs(
        self, epochs: int, current_round: Optional[int] = None
    ) -> Tuple[Dict[str, float], Dict[str, Scalar]]:
        self.model.train()
        local_step = 0
        previous_loss = float("inf")
        for local_epoch in range(epochs):
            self.train_metric_manager.clear()
            self.train_loss_meter.clear()
            for input, target in self.train_loader:
                if self.is_empty_batch(input):
                    log(INFO, "Empty batch generated by data loader. Skipping step.")
                    continue

                input, target = self._move_input_data_to_device(input), target.to(self.device)
                losses, preds = self.train_step(input, target)
                self.train_loss_meter.update(losses)
                self.train_metric_manager.update(preds, target)
                self.update_after_step(local_step)
                self.total_steps += 1
                local_step += 1

            metrics = self.train_metric_manager.compute()
            loss_dict = self.train_loss_meter.compute().as_dict()
            current_loss = loss_dict.get("backward", 0.0)

            # Early stopping check
            if self.gamma is not None and abs(previous_loss - current_loss) < self.gamma / local_epoch:
                log(
                    INFO, f"Early stopping at epoch {local_epoch} with loss change {abs(previous_loss - current_loss)}"
                )
                break

            previous_loss = current_loss

            self._handle_logging(loss_dict, metrics, current_round=current_round, current_epoch=local_epoch)
            self._handle_reporting(loss_dict, metrics, current_round=current_round)

        return loss_dict, metrics

    def train_by_steps(
        self, steps: int, current_round: Optional[int] = None
    ) -> Tuple[Dict[str, float], Dict[str, Scalar]]:
        self.model.train()

        # Pass loader to iterator so we can step through train loader
        train_iterator = iter(self.train_loader)

        self.train_loss_meter.clear()
        self.train_metric_manager.clear()
        previous_loss = float("inf")
        for step in range(steps):
            try:
                input, target = next(train_iterator)
            except StopIteration:
                train_iterator = iter(self.train_loader)
                input, target = next(train_iterator)

            if self.is_empty_batch(input):
                log(INFO, "Empty batch generated by data loader. Skipping step.")
                continue

            input, target = self._move_input_data_to_device(input), target.to(self.device)
            losses, preds = self.train_step(input, target)
            self.train_loss_meter.update(losses)
            self.train_metric_manager.update(preds, target)
            self.update_after_step(step)
            self.total_steps += 1

            loss_dict = self.train_loss_meter.compute().as_dict()
            current_loss = loss_dict.get("backward", 0.0)

            # Early stopping check
            if self.gamma is not None and abs(previous_loss - current_loss) < self.gamma / step:
                log(INFO, f"Early stopping at step {step} with loss change {abs(previous_loss - current_loss)}")
                break

            previous_loss = current_loss

        loss_dict = self.train_loss_meter.compute().as_dict()
        metrics = self.train_metric_manager.compute()

        self._handle_logging(loss_dict, metrics, current_round=current_round)
        self._handle_reporting(loss_dict, metrics, current_round=current_round)

        return loss_dict, metrics

    def setup_client(self, config: Config) -> None:
        super().setup_client(config)
        self.gamma = self.narrow_config_type(config, "gamma", float) if "gamma" in config else 0.04
