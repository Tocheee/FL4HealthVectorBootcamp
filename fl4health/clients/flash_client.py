import torch
from flwr.client import NumPyClient
from typing import Dict, Tuple, Optional
from flwr.common.typing import NDArrays, Scalar
from torch.utils.data import DataLoader

from logging import INFO
from pathlib import Path
from typing import Any, Dict, Optional, Sequence, Tuple, Type, TypeVar, Union

import torch
import torch.nn as nn
from fl4health.clients.basic_client import BasicClient
from flwr.common.logger import log
from flwr.common.typing import Config, NDArrays, Scalar
from torch.nn.modules.loss import _Loss
from torch.optim import Optimizer
from torch.utils.data import DataLoader
from fl4health.parameter_exchange.parameter_exchanger_base import ParameterExchanger

from pathlib import Path
from typing import Dict, Optional, Sequence, Tuple

import torch
from flwr.common.typing import Config, NDArrays
from opacus.optimizers.optimizer import DPOptimizer

from fl4health.checkpointing.client_module import ClientCheckpointModule
from fl4health.clients.basic_client import BasicClient, TorchInputType
from fl4health.parameter_exchange.packing_exchanger import ParameterExchangerWithPacking
from fl4health.parameter_exchange.parameter_packer import ParameterPackerWithControlVariates
from fl4health.utils.losses import LossMeterType, TrainingLosses
from fl4health.utils.metrics import Metric


class FlashClient(BasicClient):
    def __init__(
        self,
        data_path: Path,
        metrics: Sequence[Metric],
        device: torch.device,
        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,
        checkpointer: Optional[ClientCheckpointModule] = None,
        gamma: Optional[float] = None,
    ) -> None:
        super().__init__(
            data_path=data_path,
            metrics=metrics,
            device=device,
            loss_meter_type=loss_meter_type,
            checkpointer=checkpointer,
        )
        self.learning_rate: float
        self.gamma: Optional[float] = gamma

    def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -> None:
        assert self.model is not None

        # Unpack the parameters, assuming it includes model parameters and gamma
        model_params, gamma = self.unpack_parameters(parameters)
        self.gamma = gamma

        super().set_parameters(model_params, config, fitting_round)

    def unpack_parameters(self, parameters: NDArrays) -> Tuple[NDArrays, float]:
        # Unpack model parameters and gamma
        model_params = parameters[:-1]
        gamma = parameters[-1]
        return model_params, gamma

    def get_parameters(self, config: Config) -> NDArrays:
        assert self.model is not None and self.parameter_exchanger is not None

        model_weights = self.parameter_exchanger.push_parameters(self.model, config=config)

        # Include gamma in the parameters sent to the server
        if self.gamma is not None:
            model_weights.append(self.gamma)

        return model_weights

    def train_by_epochs(
        self, epochs: int, current_round: Optional[int] = None
    ) -> Tuple[Dict[str, float], Dict[str, Scalar]]:
        self.model.train()
        local_step = 0
        previous_loss = float('inf')
        for local_epoch in range(epochs):
            self.train_metric_manager.clear()
            self.train_loss_meter.clear()
            for input, target in self.train_loader:
                if self.is_empty_batch(input):
                    log(INFO, "Empty batch generated by data loader. Skipping step.")
                    continue

                input, target = self._move_input_data_to_device(input), target.to(self.device)
                losses, preds = self.train_step(input, target)
                self.train_loss_meter.update(losses)
                self.train_metric_manager.update(preds, target)
                self.update_after_step(local_step)
                self.total_steps += 1
                local_step += 1

            metrics = self.train_metric_manager.compute()
            loss_dict = self.train_loss_meter.compute().as_dict()
            current_loss = loss_dict.get("backward", 0.0)

            # Early stopping check
            if abs(previous_loss - current_loss) < self.gamma/local_epoch:
                log(INFO, f"Early stopping at epoch {local_epoch} with loss change {abs(previous_loss - current_loss)}")
                break

            previous_loss = current_loss

            self._handle_logging(loss_dict, metrics, current_round=current_round, current_epoch=local_epoch)
            self._handle_reporting(loss_dict, metrics, current_round=current_round)

        return loss_dict, metrics

    def train_by_steps(
        self, steps: int, current_round: Optional[int] = None
    ) -> Tuple[Dict[str, float], Dict[str, Scalar]]:
        self.model.train()

        # Pass loader to iterator so we can step through train loader
        train_iterator = iter(self.train_loader)

        self.train_loss_meter.clear()
        self.train_metric_manager.clear()
        previous_loss = float('inf')
        for step in range(steps):
            try:
                input, target = next(train_iterator)
            except StopIteration:
                train_iterator = iter(self.train_loader)
                input, target = next(train_iterator)

            if self.is_empty_batch(input):
                log(INFO, "Empty batch generated by data loader. Skipping step.")
                continue

            input, target = self._move_input_data_to_device(input), target.to(self.device)
            losses, preds = self.train_step(input, target)
            self.train_loss_meter.update(losses)
            self.train_metric_manager.update(preds, target)
            self.update_after_step(step)
            self.total_steps += 1

            loss_dict = self.train_loss_meter.compute().as_dict()
            current_loss = loss_dict.get("backward", 0.0)

            # Early stopping check
            if abs(previous_loss - current_loss) < self.gamma/step:
                log(INFO, f"Early stopping at step {step} with loss change {abs(previous_loss - current_loss)}")
                break

            previous_loss = current_loss

        loss_dict = self.train_loss_meter.compute().as_dict()
        metrics = self.train_metric_manager.compute()

        self._handle_logging(loss_dict, metrics, current_round=current_round)
        self._handle_reporting(loss_dict, metrics, current_round=current_round)

        return loss_dict, metrics

    def get_parameter_exchanger(self, config: Config) -> ParameterExchanger:
        assert self.model is not None
        model_size = len(self.model.state_dict())
        parameter_exchanger = ParameterExchangerWithPacking(ParameterPackerWithControlVariates(model_size))
        return parameter_exchanger

    def setup_client(self, config: Config) -> None:
        super().setup_client(config)
        assert isinstance(self.optimizers["global"], torch.optim.SGD)
        self.learning_rate = self.optimizers["global"].defaults["lr"]
