
import json
from pathlib import Path

import monai
from monai.transforms import Compose, EnsureType

import numpy as np

from typing import Any, Optional
from numpy import typing as npt

def z_score_norm(image: "npt.NDArray[Any]", percentile: Optional[float] = None) -> "npt.NDArray[Any]":
    """
    Z-score normalization (mean=0; stdev=1), where intensities
    below or above the given percentile are discarded.
    [Ref: DLTK]

    Parameters:
    - image: N-dimensional image to be normalized
    - percentile: cutoff percentile (between 0-50)

    Returns:
    - scaled image
    """
    image = image.astype(np.float32)

    if percentile is not None:
        # clip distribution of intensity values
        lower_bnd = np.percentile(image, 100-percentile)
        upper_bnd = np.percentile(image, percentile)
        image = np.clip(image, lower_bnd, upper_bnd)

    # perform z-score normalization
    mean = np.mean(image)
    std = np.std(image)
    if std > 0:
        return (image - mean) / std
    else:
        return image * 0.

def prepare_datagens(args, fold_id):
    """Load data sheets --> Create datasets --> Create data loaders"""

    # load datasheets
    with open(Path(args.overviews_dir) / f'PI-CAI_train-fold-{fold_id}.json') as fp:
        train_json = json.load(fp)
    with open(Path(args.overviews_dir) / f'PI-CAI_val-fold-{fold_id}.json') as fp:
        valid_json = json.load(fp)

    # load paths to images and labels
    train_data = [np.array(train_json['image_paths']), np.array(train_json['label_paths'])]
    valid_data = [np.array(valid_json['image_paths']), np.array(valid_json['label_paths'])]

    # use case-level class balance to deduce required train-time class weights
    class_ratio_t = [int(np.sum(train_json['case_label'])), int(len(train_data[0])-np.sum(train_json['case_label']))]
    class_ratio_v = [int(np.sum(valid_json['case_label'])), int(len(valid_data[0])-np.sum(valid_json['case_label']))]
    class_weights = (class_ratio_t / np.sum(class_ratio_t))

    # log dataset definition
    print('Dataset Definition:', "-"*80)
    print(f'Fold Number: {fold_id}')
    print('Data Classes:', list(np.unique(train_json['case_label'])))
    print(f'Train-Time Class Weights: {class_weights}')
    print(f'Training Samples [-:{class_ratio_t[1]};+:{class_ratio_t[0]}]: {len(train_data[1])}')
    print(f'Validation Samples [-:{class_ratio_v[1]};+:{class_ratio_v[0]}]: {len(valid_data[1])}')

    # dummy dataloader for sanity check
    pretx = [EnsureType()]
    check_ds = SimpleITKDataset(image_files=train_data[0][:args.batch_size*2],
                                seg_files=train_data[1][:args.batch_size*2],
                                transform=Compose(pretx),
                                seg_transform=Compose(pretx))
    check_loader = DataLoaderFromDataset(check_ds, batch_size=args.batch_size, num_threads=args.num_threads)
    data_pair = monai.utils.misc.first(check_loader)
    print('DataLoader - Image Shape: ', data_pair['data'].shape)
    print('DataLoader - Label Shape: ', data_pair['seg'].shape)
    print("-"*100)

    assert args.image_shape == list(data_pair['data'].shape[2:])
    assert args.num_channels == data_pair['data'].shape[1]
    assert args.num_classes == len(np.unique(train_json['case_label']))

    # actual dataloaders used at train-time
    train_ds = SimpleITKDataset(image_files=train_data[0], seg_files=train_data[1],
                                transform=Compose(pretx),  seg_transform=Compose(pretx))
    valid_ds = SimpleITKDataset(image_files=valid_data[0], seg_files=valid_data[1],
                                transform=Compose(pretx),  seg_transform=Compose(pretx))
    train_ldr = DataLoaderFromDataset(train_ds, 
        batch_size=args.batch_size, num_threads=args.num_threads, infinite=True, shuffle=True)
    valid_ldr = DataLoaderFromDataset(valid_ds, 
        batch_size=args.batch_size, num_threads=args.num_threads, infinite=False, shuffle=False)

    return train_ldr, valid_ldr, class_weights.astype(np.float32)
