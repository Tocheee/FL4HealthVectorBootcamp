{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class x(nn.Module):\n",
    "    def __init__(self, x1, x2):\n",
    "        super(x, self).__init__()\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "\n",
    "    def forward(self, input):\n",
    "        print(\"Im in x forward\")\n",
    "\n",
    "    def x_func1(self):\n",
    "        print(\"x_func1 called\", self.x1, self.x2)\n",
    "        return \n",
    "\n",
    "class y(nn.Module):\n",
    "    def __init__(self, y1, y2):\n",
    "        super(y, self).__init__()\n",
    "        self.y1 = y1\n",
    "        self.y2 = y2\n",
    "\n",
    "    def forward(self, input):\n",
    "        print(\"Im in y forward\")\n",
    "\n",
    "    def y_func1(self):\n",
    "        print(\"y_func1 called\", self.y1, self.y2)\n",
    "        return \n",
    "\n",
    "\n",
    "# class c(nn.Module):\n",
    "#     def __init__(self, parent, c1, c2):\n",
    "#         super(c, self).__init__()\n",
    "#         self.parent = parent\n",
    "#         self.c1 = c1\n",
    "#         self.c2 = c2\n",
    "#         self.parent_methods = {}\n",
    "        \n",
    "#         # Store the methods of the parent class in a dictionary\n",
    "#         for name in dir(parent):\n",
    "#             attr = getattr(parent, name)\n",
    "#             if callable(attr):\n",
    "#                 self.parent_methods[name] = attr\n",
    "#         print(self.parent_methods.keys())\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         out = self.parent.forward(input)\n",
    "#         print(\"Im in c forward\")\n",
    "#         return out\n",
    "\n",
    "#     def __getattr__(self, attr):\n",
    "#         if attr in self.parent_methods:\n",
    "#             return self.parent_methods[attr]\n",
    "#         else:\n",
    "#             raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{attr}'\")\n",
    "\n",
    "    # def __getattr__(self, attr):\n",
    "    #     print(\"forwarding to upper\")\n",
    "    #     if hasattr(self.parent, attr):\n",
    "    #         return getattr(self.parent, attr)\n",
    "    #     else:\n",
    "    #         raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{attr}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_my_code(base):\n",
    "    print(\"Im in get_my_code\")\n",
    "    class c(base):\n",
    "        def __init__(self,*args,c1, c2, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            print(\"Im in c init\")\n",
    "            self.c1 = c1\n",
    "            self.c2 = c2\n",
    "\n",
    "        def forward(self, input):\n",
    "            out = super().forward(input)\n",
    "            print(\"Im in c forward\")\n",
    "            return out\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im in get_my_code\n",
      "Im in c init\n"
     ]
    }
   ],
   "source": [
    "# Create a wrapper for x\n",
    "class_c = get_my_code(x)(1,2, c1=3, c2=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(class_c.c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(class_c.x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im in x forward\n",
      "Im in c forward\n"
     ]
    }
   ],
   "source": [
    "class_c.forward(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_func1 called 1 2\n"
     ]
    }
   ],
   "source": [
    "class_c.x_func1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "x.x_func1() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwrapper_for_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_func1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: x.x_func1() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "wrapper_for_x.x_func1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im in get_my_code\n",
      "Im in c init\n"
     ]
    }
   ],
   "source": [
    "# Create a wrapper for x\n",
    "wrapper_for_y = get_my_code(y)\n",
    "class_c = wrapper_for_y(1,2, c1=3, c2=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_func1 called 1 2\n"
     ]
    }
   ],
   "source": [
    "class_c.y_func1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im in y forward\n",
      "Im in c forward\n"
     ]
    }
   ],
   "source": [
    "class_c.forward(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model class defined!\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch libraries\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Other libraries we'll use\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Our images are RGB, so input channels = 3. We'll apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We'll apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 12 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A third convolutional layer takes 12 inputs and generates 24 outputs\n",
    "        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # So our feature tensors are now 32 x 32, and we've generated 24 of them\n",
    "        # We need to flatten these and feed them to a fully-connected layer\n",
    "        # to map them to  the probability for each class\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use a relu activation function after layer 1 (convolution 1 and pool)\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "      \n",
    "        # Use a relu activation function after layer 2 (convolution 2 and pool)\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        \n",
    "        # Select some features to drop after the 3rd convolution to prevent overfitting\n",
    "        x = F.relu(self.drop(self.conv3(x)))\n",
    "        \n",
    "        # Only drop the features if this is a training pass\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        # Return log_softmax tensor \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "print(\"CNN model class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def print_hello(self):\n",
    "        print(\"hello\")\n",
    "\n",
    "    def print_string(self, string):\n",
    "        print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "foo\n",
      "hello 2\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class Container:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return getattr(self.data, attr)\n",
    "    \n",
    "    def print_string(self, string):\n",
    "        self.data.print_string(string)\n",
    "        print(\"hello 2\")\n",
    "    \n",
    "class Data(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = 1\n",
    "\n",
    "    def print_hello(self):\n",
    "        print(\"hello\")\n",
    "\n",
    "    def print_string(self, string):\n",
    "        print(string)\n",
    "\n",
    "\n",
    "c = Container(Data())\n",
    "c.print_hello()\n",
    "c.print_string('foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Dict, Callable, Any, TypeVar\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_feature_extractor_model(base_model: nn.Module) -> Callable:\n",
    "    class FeatureExtractorModel(base_model):\n",
    "        def __init__(self, *args:Any, output_layers: List[str], flatten_features: Optional[List[bool]], **kwargs:Any)-> None:\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.output_layers = output_layers\n",
    "            if flatten_features is None:\n",
    "                self.flatten_features = [True] * len(output_layers)\n",
    "            else:\n",
    "                self.flatten_features = flatten_features\n",
    "            if len(self.flatten_features) != len(self.output_layers):\n",
    "                raise ValueError(\"flatten_features must be the same length as output_layers\")\n",
    "            self.selected_out: Dict = OrderedDict()\n",
    "            self.fhooks = []\n",
    "            named_layers = dict(super().named_modules())\n",
    "            for i,layer in enumerate(named_layers.keys()):\n",
    "                if i in self.output_layers:\n",
    "                    self.fhooks.append(getattr(super(),layer).register_forward_hook(self.forward_hook(layer)))\n",
    "        \n",
    "        def forward_hook(self,layer_name:str) -> Callable:\n",
    "            def hook(module: nn.Module, input:torch.Tensor, output:torch.Tensor) -> None:\n",
    "                self.selected_out[layer_name] = output\n",
    "            return hook\n",
    "\n",
    "        def flatten(self, features: torch.Tensor) -> torch.Tensor:\n",
    "            # The features are of shape (batch_size, *). We flatten them accross the batch dimension\n",
    "            # to get a 2D tensor of shape (batch_size, feature_size).\n",
    "            return features.reshape(len(features), -1)\n",
    "\n",
    "        \n",
    "        def forward(self, input: torch.Tensor) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:\n",
    "            output = super().forward(input)\n",
    "\n",
    "            out, features = (output, None) if not isinstance(output, tuple) else output\n",
    "\n",
    "            features = {} if features is None else features\n",
    "\n",
    "            for i, layer in enumerate(self.selected_out):\n",
    "                features[\" \".join([layer, \"features\"])] = self.flatten(self.selected_out[layer]) if self.flatten_features[layer] else self.selected_out[layer]\n",
    "\n",
    "            return out, features\n",
    "    return FeatureExtractorModel\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
